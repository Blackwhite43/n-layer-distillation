{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import models, losses, evaluation\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, util, InputExample\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_name = \"stsb-bert-base\"\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "output_path = \"distilled/model-distillation-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "checkpoint_path = \"checkpoint/model-distillation-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer_module in enumerate(teacher_model._first_module().auto_model.encoder.layer):\n",
    "    print(i, layer_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_layer_reduction = True\n",
    "\n",
    "# There are two options to create a light and fast student model:\n",
    "if use_layer_reduction:\n",
    "    # 1) Create a smaller student model by using only some of the teacher layers\n",
    "    student_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "    # Get the transformer model\n",
    "    auto_model = student_model._first_module().auto_model\n",
    "\n",
    "    # Which layers to keep from the teacher model. We equally spread the layers to keep over the original teacher\n",
    "    layers_to_keep = [0, 5, 7, 9, 10, 11]\n",
    "    # layers_to_keep = [0, 4, 10, 11]\n",
    "\n",
    "    logging.info(\"Remove layers from student. Only keep these layers: {}\".format(layers_to_keep))\n",
    "    new_layers = torch.nn.ModuleList(\n",
    "        [layer_module for i, layer_module in enumerate(auto_model.encoder.layer) if i in layers_to_keep]\n",
    "    )\n",
    "    auto_model.encoder.layer = new_layers\n",
    "    auto_model.config.num_hidden_layers = len(layers_to_keep)\n",
    "else:\n",
    "    # 2) The other option is to train a small model like TinyBERT to imitate the teacher.\n",
    "    # You can find some small BERT models here: https://huggingface.co/nreimers\n",
    "    word_embedding_model = models.Transformer(\"nreimers/TinyBERT_L-4_H-312_v2\")\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "    student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer_module in enumerate(student_model._first_module().auto_model.encoder.layer):\n",
    "    print(i, layer_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_batch_size = 64\n",
    "train_batch_size = 64\n",
    "\n",
    "\n",
    "# We use AllNLI as a source of sentences for the distillation\n",
    "nli_dataset_path = \"datasets/AllNLI.tsv.gz\"\n",
    "\n",
    "# Further, we use sentences extracted from the English Wikipedia to train the distillation\n",
    "wikipedia_dataset_path = \"datasets/wikipedia-en-sentences.txt.gz\"\n",
    "\n",
    "# We use the STS benchmark dataset to see how much performance we loose\n",
    "sts_dataset_path = \"datasets/stsbenchmark.tsv.gz\"\n",
    "\n",
    "\n",
    "# Download datasets if needed\n",
    "if not os.path.exists(nli_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/AllNLI.tsv.gz\", nli_dataset_path)\n",
    "\n",
    "if not os.path.exists(wikipedia_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/wikipedia-en-sentences.txt.gz\", wikipedia_dataset_path)\n",
    "\n",
    "if not os.path.exists(sts_dataset_path):\n",
    "    util.http_get(\"https://sbert.net/datasets/stsbenchmark.tsv.gz\", sts_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need sentences to train our distillation. Here, we use sentences from AllNLI and from WikiPedia\n",
    "train_sentences_nli = set()\n",
    "dev_sentences_nli = set()\n",
    "\n",
    "train_sentences_wikipedia = []\n",
    "dev_sentences_wikipedia = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ALLNLI\n",
    "with gzip.open(nli_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"dev\":\n",
    "            dev_sentences_nli.add(row[\"sentence1\"])\n",
    "            dev_sentences_nli.add(row[\"sentence2\"])\n",
    "        else:\n",
    "            train_sentences_nli.add(row[\"sentence1\"])\n",
    "            train_sentences_nli.add(row[\"sentence2\"])\n",
    "\n",
    "train_sentences_nli = list(train_sentences_nli)\n",
    "random.shuffle(train_sentences_nli)\n",
    "\n",
    "dev_sentences_nli = list(dev_sentences_nli)\n",
    "random.shuffle(dev_sentences_nli)\n",
    "dev_sentences_nli = dev_sentences_nli[0:5000]  # Limit dev sentences to 5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sentences_nli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Wikipedia sentences file\n",
    "with gzip.open(wikipedia_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    wikipeda_sentences = [line.strip() for line in fIn]\n",
    "\n",
    "dev_sentences_wikipedia = wikipeda_sentences[0:5000]  # Use the first 5k sentences from the wikipedia file for development\n",
    "train_sentences_wikipedia = wikipeda_sentences[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sentences_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the STS benchmark dataset to measure the performance of student model im comparison to the teacher model\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "with gzip.open(sts_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"dev\":\n",
    "            score = float(row[\"score\"]) / 5.0  # Normalize score to range 0 ... 1\n",
    "            dev_samples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]], label=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dev_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_evaluator_sts = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name=\"sts-dev\")\n",
    "\n",
    "logging.info(\"Teacher Performance:\")\n",
    "dev_evaluator_sts(student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(teacher_model.get_sentence_embedding_dimension(), student_model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student model has fewer dimensions. Compute PCA for the teacher to reduce the dimensions\n",
    "if student_model.get_sentence_embedding_dimension() < teacher_model.get_sentence_embedding_dimension():\n",
    "    logging.info(\"Student model has fewer dimensions than the teacher. Compute PCA for down projection\")\n",
    "    pca_sentences = train_sentences_nli[0:20000] + train_sentences_wikipedia[0:20000]\n",
    "    pca_embeddings = teacher_model.encode(pca_sentences, convert_to_numpy=True)\n",
    "    pca = PCA(n_components=student_model.get_sentence_embedding_dimension())\n",
    "    pca.fit(pca_embeddings)\n",
    "\n",
    "    # Add Dense layer to teacher that projects the embeddings down to the student embedding size\n",
    "    dense = models.Dense(\n",
    "        in_features=teacher_model.get_sentence_embedding_dimension(),\n",
    "        out_features=student_model.get_sentence_embedding_dimension(),\n",
    "        bias=False,\n",
    "        activation_function=torch.nn.Identity(),\n",
    "    )\n",
    "    dense.linear.weight = torch.nn.Parameter(torch.tensor(pca.components_))\n",
    "    teacher_model.add_module(\"dense\", dense)\n",
    "\n",
    "    logging.info(\"Teacher Performance with {} dimensions:\".format(teacher_model.get_sentence_embedding_dimension()))\n",
    "    dev_evaluator_sts(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train the student_model such that it creates sentence embeddings similar to the embeddings from the teacher_model\n",
    "# For this, we need a large set of sentences. These sentences are embedded using the teacher model,\n",
    "# and the student tries to mimic these embeddings. It is the same approach as used in: https://arxiv.org/abs/2004.09813\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=False)\n",
    "train_data.add_dataset([[data] for data in train_sentences_nli[0:5000]], max_sentence_length=256)\n",
    "train_data.add_dataset([[data] for data in train_sentences_wikipedia[0:5000]], max_sentence_length=256)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)\n",
    "\n",
    "# We create an evaluator, that measure the Mean Squared Error (MSE) between the teacher and the student embeddings\n",
    "dev_sentences = dev_sentences_nli + dev_sentences_wikipedia\n",
    "dev_evaluator_mse = evaluation.MSEEvaluator(dev_sentences, dev_sentences, teacher_model=teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the student model to imitate the teacher\n",
    "student_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluation.SequentialEvaluator([dev_evaluator_sts, dev_evaluator_mse]),\n",
    "    epochs=3,\n",
    "    warmup_steps=1000,\n",
    "    evaluation_steps=5000,\n",
    "    output_path=output_path,\n",
    "    save_best_model=True,\n",
    "    optimizer_params={\"lr\": 1e-5},\n",
    "    use_amp=True,\n",
    "    show_progress_bar=True,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    checkpoint_save_steps=3000\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-cuda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
