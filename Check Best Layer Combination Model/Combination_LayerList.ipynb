{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sentence_transformers import LoggingHandler, SentenceTransformer, InputExample, evaluation\n",
    "from sentence_transformers import models, losses\n",
    "import logging\n",
    "import gzip\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\", level=logging.INFO, handlers=[LoggingHandler()])\n",
    "teacher_model_name = \"stsb-bert-base\"\n",
    "teacher_model = SentenceTransformer(teacher_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the STS benchmark dataset to see how much performance we loose\n",
    "sts_dataset_path = \"datasets/stsbenchmark.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the STS benchmark dataset to measure the performance of student model im comparison to the teacher model\n",
    "logging.info(\"Read STSbenchmark dev dataset\")\n",
    "dev_samples = []\n",
    "with gzip.open(sts_dataset_path, \"rt\", encoding=\"utf8\") as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        if row[\"split\"] == \"dev\":\n",
    "            score = float(row[\"score\"]) / 5.0  # Normalize score to range 0 ... 1\n",
    "            dev_samples.append(InputExample(texts=[row[\"sentence1\"], row[\"sentence2\"]], label=score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numbers from 0 to 11\n",
    "numbers = list(range(12))\n",
    "\n",
    "# Generate all combinations of 4 numbers\n",
    "combinations = list(itertools.combinations(numbers, 4))\n",
    "\n",
    "# Generate all combinations of 6 numbers\n",
    "# combinations = list(itertools.combinations(numbers, 6))\n",
    "\n",
    "# Convert the combinations to a NumPy array\n",
    "combinations_np = np.array(combinations)\n",
    "\n",
    "# Print the NumPy array\n",
    "combinations_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combinations_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for combo in combinations_np:\n",
    "    student_model = SentenceTransformer(teacher_model_name)\n",
    "    auto_model = student_model._first_module().auto_model\n",
    "    \n",
    "    layers_to_keep = combo\n",
    "    logging.info(\"Remove layers from student. Only keep these layers: {}\".format(layers_to_keep))\n",
    "\n",
    "    new_layers = torch.nn.ModuleList(\n",
    "        [layer_module for i, layer_module in enumerate(auto_model.encoder.layer) if i in layers_to_keep]\n",
    "    )\n",
    "\n",
    "    auto_model.encoder.layer = new_layers\n",
    "    auto_model.config.num_hidden_layers = len(layers_to_keep)\n",
    "    \n",
    "    dev_evaluator_sts = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name=\"sts-dev\")\n",
    "    evaluator_results = dev_evaluator_sts(student_model)\n",
    "    logging.info(\"Teacher Performance:\")\n",
    "\n",
    "    data.append([\n",
    "        combo,\n",
    "        evaluator_results[\"sts-dev_spearman_cosine\"],\n",
    "        evaluator_results[\"sts-dev_pearson_cosine\"],\n",
    "        evaluator_results[\"sts-dev_spearman_manhattan\"],\n",
    "        evaluator_results[\"sts-dev_pearson_manhattan\"],\n",
    "        evaluator_results[\"sts-dev_spearman_euclidean\"],\n",
    "        evaluator_results[\"sts-dev_pearson_euclidean\"],\n",
    "        evaluator_results[\"sts-dev_spearman_dot\"],\n",
    "        evaluator_results[\"sts-dev_pearson_dot\"]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"layer\",\n",
    "    \"spearman_cosine\",\n",
    "    \"pearson_cosine\",\n",
    "    \"spearman_manhattan\",\n",
    "    \"pearson_manhattan\",\n",
    "    \"spearman_euclidean\",\n",
    "    \"pearson_euclidean\",\n",
    "    \"spearman_dot\",\n",
    "    \"pearson_dot\"\n",
    "]\n",
    "excel = pd.DataFrame(data, columns=cols)\n",
    "excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining the name of the file\n",
    "file_name = 'Combination_Layer_Results.xlsx'\n",
    "\n",
    "# saving the excel\n",
    "excel.to_excel(file_name, index=False)\n",
    "print('DataFrame is written to Excel File successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
